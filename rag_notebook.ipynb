{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfd9361-ebfc-4d1d-bf48-aadf82e42734",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8211e2c7-5872-4303-aa89-4b08d150630e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list\n",
    "# __import__('pysqlite3')\n",
    "# import pysqlite3\n",
    "# sys.modules['sqlite3'] = sys.modules[\"pysqlite3\"]\n",
    "# import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac907ce8-8118-4ecb-a0a9-749c73e42c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoConfig,\n",
    ")\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "import warnings\n",
    "from IPython.display import Markdown\n",
    "import re\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928b93ac-5fbe-4fd1-b648-44174c4168c2",
   "metadata": {},
   "source": [
    "## Ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fae8804-6107-4b9a-94ac-0516ef47de5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "\n",
    "# List of WCA URLs for the loader.\n",
    "urls = [\n",
    "    \"https://www.worldcubeassociation.org/regulations/\",\n",
    "    \"https://www.worldcubeassociation.org/regulations/guidelines.html\",\n",
    "    \"https://www.worldcubeassociation.org/regulations/scrambles/\"\n",
    "]\n",
    "\n",
    "# Defining the URL Loader\n",
    "loader = UnstructuredURLLoader(urls=urls)\n",
    "\n",
    "# Loading the data\n",
    "data = loader.load()\n",
    "\n",
    "# Pre-processing the data using regex\n",
    "data[0].page_content = re.sub(\"\\n{3,}\", \"\\n\", data[0].page_content)\n",
    "data[0].page_content = re.sub(\" {2,}\", \" \", data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fc90b25c-7304-4cce-84c2-586ad103f716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Inference Server Setup\\n\\nThe inference server is responsible for performing text inference tasks using Hugging Face\\'s Large Language Models. You need to specify the URL of the inference server that the application will communicate with.\\n\\nINFERENCE_SERVER_URL should be set to the URL of your Hugging Face inference server. If you\\'re running the server locally for testing, you can use \"http://localhost:3000/\". For production or cloud environments, you would replace this with the actual URL of your deployed inference server.\\n\\nKafka Setup\\n\\nThis application uses Kafka for message queueing, consuming messages from a chat topic, processing them, and then producing responses to an answer topic.\\n\\nKAFKA_SERVER specifies the address of your Kafka server. If running locally, it\\'s typically set to \"localhost:9092\". For production, this would be the address of your Kafka cluster.', metadata={'source': 'content/readme.md'}),\n",
       " Document(page_content='KAFKA_SERVER specifies the address of your Kafka server. If running locally, it\\'s typically set to \"localhost:9092\". For production, this would be the address of your Kafka cluster.\\n\\nCONSUMER_TOPIC is the name of the Kafka topic from which the application will consume messages. This should be set to \"chat\" or whichever topic you have designated for incoming chat messages.\\n\\nPRODUCER_TOPIC is the name of the Kafka topic to which the application will produce processed messages. This is set to \"answer\", or any other topic name where you want the processed messages to be published.\\n\\nEnsure these settings are correctly configured to match your environment before running the application.', metadata={'source': 'content/readme.md'})]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "# load from documents directory\n",
    "loader = DirectoryLoader('./content/', glob=\"**/*.md\")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "texts = text_splitter.split_documents(docs)\n",
    "\n",
    "len(texts)\n",
    "\n",
    "vectordb = FAISS.from_documents(documents=texts,\n",
    "                                 embedding=embeddings)\n",
    "\n",
    "retriever = vectordb.as_retriever(search_kwargs={'k': 2})\n",
    "\n",
    "rdocs = retriever.get_relevant_documents(\"chatbot\")\n",
    "rdocs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9792bc-d293-4aa3-a1e2-8a5d61335407",
   "metadata": {},
   "source": [
    "## Document Splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993456de-cffe-470a-92c1-3ad3a36dcf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    ")\n",
    "\n",
    "# Using the recursive character splitter\n",
    "recur_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=60,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"],\n",
    "    is_separator_regex=True,\n",
    ")\n",
    "\n",
    "# Performing the splits using the splitter\n",
    "data_splits = recur_splitter.split_documents(data)\n",
    "\n",
    "# Printing a random chunk\n",
    "print(random.choice(data_splits).page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b6c797-8fb9-49de-b803-3ee380ec95f5",
   "metadata": {},
   "source": [
    "## Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326766dc-9d50-4d16-97de-51e395060d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "### Using embeddings by MPNET\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": False}\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd17db0-f68c-4be4-a699-13015155262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import vectorstore\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Define the location to persist data\n",
    "persist_directory = \"chroma/\"\n",
    "!rm -rf chroma\n",
    "\n",
    "# Generate and store embeddings\n",
    "vectordb = FAISS.from_documents(\n",
    "    documents=data_splits, embedding=hf_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f33cd07-869e-40d5-8f03-7981d9b61ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to retrieve similar chunks\n",
    "query = \"Are hand warmers considered as electronic devices?\"\n",
    "\n",
    "# Retrieve similar chunks based on relevance. We only retrieve 'k' most similar chunks\n",
    "similar_chunks = vectordb.similarity_search_with_relevance_scores(query, k=3)\n",
    "\n",
    "# Format document to text format\n",
    "retrieved_text = [chunk[0].page_content for chunk in similar_chunks]\n",
    "relevance_score = [chunk[1] for chunk in similar_chunks]\n",
    "\n",
    "# Store and print as a dataframe\n",
    "retrieved_chunks = pd.DataFrame(\n",
    "    list(zip(retrieved_text, relevance_score)),\n",
    "    columns=[\"Retrieved Chunks\", \"Relevance Score\"],\n",
    ")\n",
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    display(retrieved_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35da47c0-d826-4c3b-8957-3091e3c351b4",
   "metadata": {},
   "source": [
    "## LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3b15c5d5-f1e9-40cd-99ae-f28c35a31e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFaceTextGenInference\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "\n",
    "llm = HuggingFaceTextGenInference(\n",
    "            inference_server_url=\"https://hf-tgi-server-llms.apps.cluster-45cdc.45cdc.openshift.opentlc.com\",\n",
    "            max_new_tokens=512,\n",
    "            top_k=10,\n",
    "            top_p=0.95,\n",
    "            typical_p=0.95,\n",
    "            temperature=0.1,\n",
    "            repetition_penalty=1.175\n",
    ")\n",
    "\n",
    "llm_template = \"\"\" Answer the question below.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. Never Hallucinate.\n",
    "Keep the answer as concise as possible.\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "qa_prompt_template = PromptTemplate.from_template(llm_template)\n",
    "\n",
    "# retriever = vectordb.as_retriever(search_kwargs=dict(k=1))\n",
    "memory = VectorStoreRetrieverMemory(retriever=retriever)\n",
    "chain = LLMChain(llm=llm, prompt=qa_prompt_template, verbose=True, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "17440d28-c9c0-43f8-8045-6cdd19380ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m Answer the question below.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer. Never Hallucinate.\n",
      "Keep the answer as concise as possible.\n",
      "\n",
      "Question: Why the application needs Kafka?\n",
      "Answer:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Why the application needs Kafka?',\n",
       " 'history': 'KAFKA_SERVER specifies the address of your Kafka server. If running locally, it\\'s typically set to \"localhost:9092\". For production, this would be the address of your Kafka cluster.\\n\\nCONSUMER_TOPIC is the name of the Kafka topic from which the application will consume messages. This should be set to \"chat\" or whichever topic you have designated for incoming chat messages.\\n\\nPRODUCER_TOPIC is the name of the Kafka topic to which the application will produce processed messages. This is set to \"answer\", or any other topic name where you want the processed messages to be published.\\n\\nEnsure these settings are correctly configured to match your environment before running the application.\\nInference Server Setup\\n\\nThe inference server is responsible for performing text inference tasks using Hugging Face\\'s Large Language Models. You need to specify the URL of the inference server that the application will communicate with.\\n\\nINFERENCE_SERVER_URL should be set to the URL of your Hugging Face inference server. If you\\'re running the server locally for testing, you can use \"http://localhost:3000/\". For production or cloud environments, you would replace this with the actual URL of your deployed inference server.\\n\\nKafka Setup\\n\\nThis application uses Kafka for message queueing, consuming messages from a chat topic, processing them, and then producing responses to an answer topic.\\n\\nKAFKA_SERVER specifies the address of your Kafka server. If running locally, it\\'s typically set to \"localhost:9092\". For production, this would be the address of your Kafka cluster.',\n",
       " 'text': 'Kafka is a distributed streaming platform that allows applications to process large volumes of data in real-time. It is used to store and process data streams in a scalable and fault-tolerant way. It is also used to process data streams in a distributed manner, which is necessary for applications that process large amounts of data.'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"Why the application needs Kafka?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4452de37-f592-4926-820b-436c0bacb609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
