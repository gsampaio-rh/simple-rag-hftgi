{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfd9361-ebfc-4d1d-bf48-aadf82e42734",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac907ce8-8118-4ecb-a0a9-749c73e42c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Importing necessary libraries and handling warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Ignore warnings to keep notebook clean\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoConfig,\n",
    ")\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "from IPython.display import Markdown\n",
    "import re\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928b93ac-5fbe-4fd1-b648-44174c4168c2",
   "metadata": {},
   "source": [
    "## Ingest\n",
    "This section is dedicated to loading and preparing data for processing. Here, we specifically handle markdown files, loading them from a local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc90b25c-7304-4cce-84c2-586ad103f716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "# Loading markdown files from the specified directory\n",
    "# Using a glob pattern to select all markdown files recursively\n",
    "loader_md = DirectoryLoader(\"./content/\", glob=\"**/*.md\")\n",
    "try:\n",
    "    md_data = loader_md.load()\n",
    "    print(\"Markdown files loaded successfully:\", len(md_data))\n",
    "except Exception as e:\n",
    "    print(\"Failed to load markdown files:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9792bc-d293-4aa3-a1e2-8a5d61335407",
   "metadata": {},
   "source": [
    "## Document Splitters\n",
    "In this section, we define how documents are split into manageable parts for further analysis or processing. This is crucial for handling large texts efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993456de-cffe-470a-92c1-3ad3a36dcf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    ")\n",
    "\n",
    "# Setting up the Recursive Character Text Splitter\n",
    "recur_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=60,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"],\n",
    "    is_separator_regex=True,\n",
    ")\n",
    "\n",
    "# Assume md_data is a list of some document objects\n",
    "# Use the regex preprocessing and splitter for markdown data\n",
    "for doc in md_data:\n",
    "    doc.page_content = re.sub(\"\\n{3,}\", \"\\n\", doc.page_content)\n",
    "    doc.page_content = re.sub(\" {2,}\", \" \", doc.page_content)\n",
    "\n",
    "# Splitting Markdown documents\n",
    "md_data_splits = recur_splitter.split_documents(md_data)\n",
    "\n",
    "len(md_data_splits)\n",
    "\n",
    "# Print a random chunk from Markdown content\n",
    "print(random.choice(md_data_splits).page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b6c797-8fb9-49de-b803-3ee380ec95f5",
   "metadata": {},
   "source": [
    "## Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90a9656-3183-4f1e-807e-5cc37099b40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "### Using embeddings by MPNET\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": False}\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d1be6d-738c-4c2a-9733-51f284a0ecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import vectorstore\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Generate and store embeddings\n",
    "vectordb = FAISS.from_documents(documents=md_data_splits, embedding=hf_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f96adbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'rdocs' is a list of Document objects as shown above\n",
    "def print_retrieved_documents(documents):\n",
    "    for idx, doc in enumerate(documents, start=1):\n",
    "        print(f\"Document {idx}:\")\n",
    "        print(f\"Source: {doc.metadata.get('source')}\\n\")\n",
    "        # Splitting the content into paragraphs for better readability\n",
    "        paragraphs = doc.page_content.split(\"\\n\\n\")\n",
    "        for paragraph in paragraphs:\n",
    "            print(paragraph)\n",
    "        print(\"\\n\" + \"-\" * 80 + \"\\n\")  # Add a separator line between documents\n",
    "\n",
    "# Retrieve documents using a retriever from vectordb\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 2})\n",
    "rdocs = retriever.get_relevant_documents(\"chatbot\")\n",
    "\n",
    "# Improved print of retrieved documents\n",
    "print_retrieved_documents(rdocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c64e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to retrieve similar chunks\n",
    "query = \"Kafka?\"\n",
    "\n",
    "# Retrieve similar chunks based on relevance. We only retrieve 'k' most similar chunks\n",
    "similar_chunks = vectordb.similarity_search_with_relevance_scores(query, k=3)\n",
    "\n",
    "# Format document to text format\n",
    "retrieved_text = [chunk[0].page_content for chunk in similar_chunks]\n",
    "relevance_score = [chunk[1] for chunk in similar_chunks]\n",
    "\n",
    "# Store and print as a dataframe\n",
    "retrieved_chunks = pd.DataFrame(\n",
    "    list(zip(retrieved_text, relevance_score)),\n",
    "    columns=[\"Retrieved Chunks\", \"Relevance Score\"],\n",
    ")\n",
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    display(retrieved_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35da47c0-d826-4c3b-8957-3091e3c351b4",
   "metadata": {},
   "source": [
    "## LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b15c5d5-f1e9-40cd-99ae-f28c35a31e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFaceTextGenInference\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "\n",
    "llm = HuggingFaceTextGenInference(\n",
    "            inference_server_url=\"https://hf-tgi-server-llms.apps.cluster-45cdc.45cdc.openshift.opentlc.com\",\n",
    "            max_new_tokens=512,\n",
    "            top_k=10,\n",
    "            top_p=0.95,\n",
    "            typical_p=0.95,\n",
    "            temperature=0.1,\n",
    "            repetition_penalty=1.175\n",
    ")\n",
    "\n",
    "llm_template = \"\"\" Answer the question below.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. Never Hallucinate.\n",
    "Keep the answer as concise as possible.\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "qa_prompt_template = PromptTemplate.from_template(llm_template)\n",
    "\n",
    "# retriever = vectordb.as_retriever(search_kwargs=dict(k=1))\n",
    "memory = VectorStoreRetrieverMemory(retriever=retriever)\n",
    "chain = LLMChain(llm=llm, prompt=qa_prompt_template, verbose=True, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17440d28-c9c0-43f8-8045-6cdd19380ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"question\": \"Why the application needs Kafka?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
