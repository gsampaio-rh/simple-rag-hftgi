{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfd9361-ebfc-4d1d-bf48-aadf82e42734",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac907ce8-8118-4ecb-a0a9-749c73e42c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Importing necessary libraries and handling warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Ignore warnings to keep notebook clean\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoConfig,\n",
    ")\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "from IPython.display import Markdown\n",
    "import re\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928b93ac-5fbe-4fd1-b648-44174c4168c2",
   "metadata": {},
   "source": [
    "## Ingest\n",
    "This section is dedicated to loading and preparing data for processing. Here, we specifically handle markdown files, loading them from a local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc90b25c-7304-4cce-84c2-586ad103f716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading markdown files from the specified directory\n",
    "# Using a glob pattern to select all markdown files recursively\n",
    "loader_md = DirectoryLoader(\"./content/\", glob=\"**/*.md\")\n",
    "try:\n",
    "    md_data = loader_md.load()\n",
    "    print(\"Markdown files loaded successfully:\", len(md_data))\n",
    "except Exception as e:\n",
    "    print(\"Failed to load markdown files:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9792bc-d293-4aa3-a1e2-8a5d61335407",
   "metadata": {},
   "source": [
    "## Document Splitters\n",
    "In this section, we define how documents are split into manageable parts for further analysis or processing. This is crucial for handling large texts efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993456de-cffe-470a-92c1-3ad3a36dcf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Importing necessary libraries and handling warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Ignore warnings to keep notebook clean\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoConfig,\n",
    ")\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "from IPython.display import Markdown\n",
    "import re\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b6c797-8fb9-49de-b803-3ee380ec95f5",
   "metadata": {},
   "source": [
    "## Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326766dc-9d50-4d16-97de-51e395060d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Importing necessary libraries and handling warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Ignore warnings to keep notebook clean\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoConfig,\n",
    ")\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "from IPython.display import Markdown\n",
    "import re\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd17db0-f68c-4be4-a699-13015155262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Importing necessary libraries and handling warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Ignore warnings to keep notebook clean\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoConfig,\n",
    ")\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "from IPython.display import Markdown\n",
    "import re\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f96adbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'rdocs' is a list of Document objects as shown above\n",
    "def print_retrieved_documents(documents):\n",
    "    for idx, doc in enumerate(documents, start=1):\n",
    "        print(f\"Document {idx}:\")\n",
    "        print(f\"Source: {doc.metadata.get('source')}\\n\")\n",
    "        # Splitting the content into paragraphs for better readability\n",
    "        paragraphs = doc.page_content.split(\"\\n\\n\")\n",
    "        for paragraph in paragraphs:\n",
    "            print(paragraph)\n",
    "        print(\"\\n\" + \"-\" * 80 + \"\\n\")  # Add a separator line between documents\n",
    "\n",
    "# Retrieve documents using a retriever from vectordb\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 2})\n",
    "rdocs = retriever.get_relevant_documents(\"chatbot\")\n",
    "\n",
    "# Improved print of retrieved documents\n",
    "print_retrieved_documents(rdocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c64e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to retrieve similar chunks\n",
    "query = \"Kafka?\"\n",
    "\n",
    "# Retrieve similar chunks based on relevance. We only retrieve 'k' most similar chunks\n",
    "similar_chunks = vectordb.similarity_search_with_relevance_scores(query, k=3)\n",
    "\n",
    "# Format document to text format\n",
    "retrieved_text = [chunk[0].page_content for chunk in similar_chunks]\n",
    "relevance_score = [chunk[1] for chunk in similar_chunks]\n",
    "\n",
    "# Store and print as a dataframe\n",
    "retrieved_chunks = pd.DataFrame(\n",
    "    list(zip(retrieved_text, relevance_score)),\n",
    "    columns=[\"Retrieved Chunks\", \"Relevance Score\"],\n",
    ")\n",
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    display(retrieved_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35da47c0-d826-4c3b-8957-3091e3c351b4",
   "metadata": {},
   "source": [
    "## LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b15c5d5-f1e9-40cd-99ae-f28c35a31e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Importing necessary libraries and handling warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Ignore warnings to keep notebook clean\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoConfig,\n",
    ")\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "from IPython.display import Markdown\n",
    "import re\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17440d28-c9c0-43f8-8045-6cdd19380ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"question\": \"Why the application needs Kafka?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4452de37-f592-4926-820b-436c0bacb609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
